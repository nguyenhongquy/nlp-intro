{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8d3d41",
   "metadata": {},
   "source": [
    "## üîÅüìä Text Classification with LSTM and GRU (AG News)\n",
    "This notebook trains both GRU and LSTM-based classifiers on AG News and evaluates them with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dccb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622fd70",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ff574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def encode(text):\n",
    "    return torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for label, text in batch:\n",
    "        encoded = encode(text)\n",
    "        label_list.append(torch.tensor(label - 1))\n",
    "        text_list.append(encoded)\n",
    "        lengths.append(len(encoded))\n",
    "    padded = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    return padded, torch.tensor(lengths), torch.tensor(label_list)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "test_iter = AG_NEWS(split='test')\n",
    "train_loader = DataLoader(list(train_iter)[:5000], batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(list(test_iter)[:1000], batch_size=64, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602590fe",
   "metadata": {},
   "source": [
    "### 2. Define GRU and LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, use_lstm=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True) if use_lstm else nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.rnn(packed)\n",
    "        if isinstance(hidden, tuple):  # LSTM returns (hidden_state, cell_state)\n",
    "            hidden = hidden[0]\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a993df7",
   "metadata": {},
   "source": [
    "### 3. Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da514fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for text, lengths, labels in loader:\n",
    "        text, lengths, labels = text.to(device), lengths.to(device), labels.to(device)\n",
    "        preds = model(text, lengths)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5c813",
   "metadata": {},
   "source": [
    "### 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e420a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for text, lengths, labels in loader:\n",
    "            text, lengths = text.to(device), lengths.to(device)\n",
    "            preds = model(text, lengths)\n",
    "            predicted = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "            all_preds.extend(predicted)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    return accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90092b76",
   "metadata": {},
   "source": [
    "### 5. Train and Evaluate GRU and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257246a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# GRU Model\n",
    "gru_model = RNNClassifier(len(vocab), 64, 128, 4, use_lstm=False).to(device)\n",
    "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_model(gru_model, train_loader, gru_optimizer, criterion, device)\n",
    "    acc = evaluate_model(gru_model, test_loader, device)\n",
    "    print(f\"GRU - Epoch {epoch+1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = RNNClassifier(len(vocab), 64, 128, 4, use_lstm=True).to(device)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_model(lstm_model, train_loader, lstm_optimizer, criterion, device)\n",
    "    acc = evaluate_model(lstm_model, test_loader, device)\n",
    "    print(f\"LSTM - Epoch {epoch+1}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
