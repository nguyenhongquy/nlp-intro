{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921ead1a",
   "metadata": {},
   "source": [
    "## ðŸ§  Text Classification with PyTorch (AG News)\n",
    "This notebook walks through training a text classification model on the AG News dataset using `torchtext` and a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2163bb0",
   "metadata": {},
   "source": [
    "### 1. Load and Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addda206",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2514c",
   "metadata": {},
   "source": [
    "### 2. Create Encoding and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d052fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    return torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for label, text in batch:\n",
    "        label_list.append(torch.tensor(label - 1))\n",
    "        text_list.append(encode(text))\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    return text_padded, torch.tensor(label_list)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "train_loader = DataLoader(list(train_iter)[:5000], batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c64d5f",
   "metadata": {},
   "source": [
    "### 3. Define the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        pooled = embedded.mean(dim=1)  # mean pooling\n",
    "        return self.fc(pooled)\n",
    "\n",
    "model = TextClassifier(len(vocab), embed_dim=64, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ccdd7",
   "metadata": {},
   "source": [
    "### 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5892dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for text, labels in train_loader:\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        preds = model(text)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cfafc7",
   "metadata": {},
   "source": [
    "### 5. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60513000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = encode(text).unsqueeze(0).to(device)\n",
    "        pred = model(encoded)\n",
    "        label = torch.argmax(pred, dim=1).item()\n",
    "    return label\n",
    "\n",
    "test_text = \"The stock market saw a sharp decline today due to economic concerns.\"\n",
    "print(\"Predicted class:\", predict(test_text))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
