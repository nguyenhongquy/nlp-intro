{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b30c28",
   "metadata": {},
   "source": [
    "## üîÅ Text Classification with RNNs (AG News)\n",
    "This notebook extends the basic text classifier by using a GRU-based RNN for improved sequence modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b006b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dfa8f",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def344bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def encode(text):\n",
    "    return torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for label, text in batch:\n",
    "        encoded = encode(text)\n",
    "        label_list.append(torch.tensor(label - 1))\n",
    "        text_list.append(encoded)\n",
    "        lengths.append(len(encoded))\n",
    "    padded = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    return padded, torch.tensor(lengths), torch.tensor(label_list)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "train_loader = DataLoader(list(train_iter)[:5000], batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c7380",
   "metadata": {},
   "source": [
    "### 2. Define GRU-based RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d16bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.gru(packed)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "model = RNNClassifier(len(vocab), embed_dim=64, hidden_dim=128, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b18eda",
   "metadata": {},
   "source": [
    "### 3. Train the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for text, lengths, labels in train_loader:\n",
    "        text, lengths, labels = text.to(device), lengths.to(device), labels.to(device)\n",
    "        preds = model(text, lengths)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb79fdd",
   "metadata": {},
   "source": [
    "### 4. Test Inference with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41379b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = encode(text)\n",
    "        length = torch.tensor([len(encoded)])\n",
    "        padded = pad_sequence([encoded], batch_first=True, padding_value=vocab['<pad>']).to(device)\n",
    "        pred = model(padded, length.to(device))\n",
    "        label = torch.argmax(pred, dim=1).item()\n",
    "    return label\n",
    "\n",
    "test_text = \"The government is planning new economic reforms.\"\n",
    "print(\"Predicted class:\", predict(test_text))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
